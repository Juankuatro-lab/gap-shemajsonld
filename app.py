import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin, urlparse, parse_qs
import random
from datetime import datetime
import json
from textblob import TextBlob
import io

# Configuration de la page Streamlit
st.set_page_config(
    page_title="Extracteur d'avis Amazon - Batch",
    page_icon="üìù",
    layout="wide"
)

class SentimentAnalyzer:
    """Analyseur de sentiment pour les commentaires"""
    
    @staticmethod
    def analyze_sentiment(text):
        """Analyse le sentiment d'un texte et retourne un label"""
        if not text or len(text.strip()) < 3:
            return "Neutre"
        
        try:
            # Utilisation de TextBlob pour l'analyse de sentiment
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            
            if polarity > 0.1:
                return "Positif"
            elif polarity < -0.1:
                return "N√©gatif"
            else:
                return "Neutre"
        except:
            return "Neutre"

class AmazonReviewsExtractor:
    def __init__(self):
        # Headers plus r√©alistes pour √©viter la d√©tection
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        
    def clean_url(self, url):
        """Nettoie l'URL pour extraire l'URL de base du produit"""
        try:
            # Extraire l'ASIN de diff√©rents formats d'URL
            patterns = [
                r'/dp/([A-Z0-9]{10})',
                r'/product/([A-Z0-9]{10})',
                r'asin=([A-Z0-9]{10})',
                r'/gp/product/([A-Z0-9]{10})',
                r'/([A-Z0-9]{10})(?:/|$)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    asin = match.group(1)
                    # D√©tecter le domaine Amazon
                    if "amazon.fr" in url:
                        return f"https://www.amazon.fr/dp/{asin}", asin
                    elif "amazon.com" in url:
                        return f"https://www.amazon.com/dp/{asin}", asin
                    elif "amazon.de" in url:
                        return f"https://www.amazon.de/dp/{asin}", asin
                    elif "amazon.co.uk" in url:
                        return f"https://www.amazon.co.uk/dp/{asin}", asin
                    else:
                        # Par d√©faut, amazon.fr
                        return f"https://www.amazon.fr/dp/{asin}", asin
            
            return None, None
        except Exception as e:
            st.error(f"Erreur lors du nettoyage de l'URL: {str(e)}")
            return None, None
    
    def get_reviews_url(self, product_url, asin):
        """Construit l'URL de la page des avis"""
        try:
            if "amazon.fr" in product_url:
                return f"https://www.amazon.fr/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1"
            elif "amazon.com" in product_url:
                return f"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1"
            elif "amazon.de" in product_url:
                return f"https://www.amazon.de/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1"
            elif "amazon.co.uk" in product_url:
                return f"https://www.amazon.co.uk/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1"
            else:
                return f"https://www.amazon.fr/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1"
        except:
            return None
    
    def extract_product_info(self, product_url):
        """Extrait les informations g√©n√©rales du produit"""
        try:
            st.write(f"üîç Extraction des infos produit depuis: {product_url}")
            
            # Pause al√©atoire
            time.sleep(random.uniform(2, 5))
            
            session = requests.Session()
            session.headers.update(self.headers)
            
            response = session.get(product_url, timeout=15)
            
            if response.status_code != 200:
                st.warning(f"‚ö†Ô∏è Code HTTP {response.status_code} pour la page produit")
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Multiples s√©lecteurs pour la note moyenne (Amazon change souvent)
            avg_rating = None
            rating_selectors = [
                'span.a-icon-alt',
                'span[data-hook="rating-out-of-text"]',
                'span.a-offscreen',
                'i.a-icon-star span.a-offscreen',
                '.a-icon-star .a-offscreen'
            ]
            
            for selector in rating_selectors:
                try:
                    rating_elem = soup.select_one(selector)
                    if rating_elem:
                        rating_text = rating_elem.text or rating_elem.get('title', '')
                        rating_match = re.search(r'(\d+(?:[,\.]\d+)?)', rating_text)
                        if rating_match:
                            avg_rating = float(rating_match.group(1).replace(',', '.'))
                            break
                except:
                    continue
            
            # Multiples s√©lecteurs pour le nombre total d'avis
            total_reviews = 0
            review_count_selectors = [
                'span[data-hook="total-review-count"]',
                'a[data-hook="see-all-reviews-link-foot"]',
                '#acrCustomerReviewText',
                'span#acrCustomerReviewText',
                '.a-link-normal[data-hook="see-all-reviews-link-foot"]'
            ]
            
            for selector in review_count_selectors:
                try:
                    reviews_elem = soup.select_one(selector)
                    if reviews_elem:
                        reviews_text = reviews_elem.text
                        # Recherche de nombres dans le texte
                        numbers = re.findall(r'(\d+(?:[\s,]\d+)*)', reviews_text.replace(',', '').replace(' ', ''))
                        if numbers:
                            total_reviews = int(numbers[0])
                            break
                except:
                    continue
            
            st.success(f"‚úÖ Info produit: {avg_rating}/5 ‚≠ê | {total_reviews} avis")
            
            return {
                'avg_rating': avg_rating,
                'total_reviews': total_reviews
            }
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erreur lors de l'extraction des infos produit: {str(e)}")
            return None
    
    def extract_single_review(self, review_element):
        """Extrait les informations d'un seul avis"""
        try:
            review_data = {}
            
            # Multiples s√©lecteurs pour la note
            rating = None
            rating_selectors = [
                'span.a-icon-alt',
                'i.a-icon-star span.a-offscreen',
                '.a-icon-star .a-offscreen',
                'span[data-hook="review-star-rating"] span.a-offscreen'
            ]
            
            for selector in rating_selectors:
                try:
                    rating_elem = review_element.select_one(selector)
                    if rating_elem:
                        rating_text = rating_elem.text or rating_elem.get('title', '')
                        rating_match = re.search(r'(\d+(?:[,\.]\d+)?)', rating_text)
                        if rating_match:
                            rating = float(rating_match.group(1).replace(',', '.'))
                            break
                except:
                    continue
            
            review_data['rating'] = rating
            
            # Multiples s√©lecteurs pour le contenu
            content = ""
            content_selectors = [
                'span[data-hook="review-body"]',
                '.review-text',
                '.cr-original-review-text',
                'div[data-hook="review-body"] span'
            ]
            
            for selector in content_selectors:
                try:
                    content_elem = review_element.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(strip=True)
                        if content and len(content) > 10:  # Au moins 10 caract√®res
                            break
                except:
                    continue
            
            review_data['content'] = content
            
            return review_data if (rating is not None or content) else None
            
        except Exception as e:
            return None
    
    def extract_reviews_for_product(self, original_url, max_pages=3):
        """Extrait les avis d'un produit sp√©cifique"""
        try:
            # Nettoyer l'URL
            clean_url, asin = self.clean_url(original_url)
            
            if not clean_url or not asin:
                st.error(f"‚ùå Impossible d'extraire l'ASIN depuis: {original_url}")
                return None
            
            st.info(f"üßπ URL nettoy√©e: {clean_url}")
            st.info(f"üÜî ASIN d√©tect√©: {asin}")
            
            # Extraction des informations g√©n√©rales du produit
            product_info = self.extract_product_info(clean_url)
            
            # URL des avis
            reviews_url = self.get_reviews_url(clean_url, asin)
            if not reviews_url:
                st.error("‚ùå Impossible de construire l'URL des avis")
                return None
            
            st.info(f"üìÑ URL des avis: {reviews_url}")
            
            reviews = []
            session = requests.Session()
            session.headers.update(self.headers)
            
            # Extraction des avis d√©taill√©s
            for page in range(1, max_pages + 1):
                try:
                    st.write(f"üìñ Extraction page {page}/{max_pages}...")
                    
                    current_url = reviews_url.replace('pageNumber=1', f'pageNumber={page}')
                    time.sleep(random.uniform(3, 6))  # Pause plus longue
                    
                    response = session.get(current_url, timeout=15)
                    
                    if response.status_code != 200:
                        st.warning(f"‚ö†Ô∏è Code HTTP {response.status_code} pour la page {page}")
                        continue
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Multiples s√©lecteurs pour les avis
                    review_selectors = [
                        'div[data-hook="review"]',
                        '.review',
                        '.cr-original-review-text',
                        'div.a-section.review'
                    ]
                    
                    review_elements = []
                    for selector in review_selectors:
                        elements = soup.select(selector)
                        if elements:
                            review_elements = elements
                            break
                    
                    if not review_elements:
                        st.warning(f"‚ö†Ô∏è Aucun avis trouv√© sur la page {page}")
                        # Essayer avec d'autres s√©lecteurs
                        all_divs = soup.find_all('div', class_=True)
                        review_elements = [div for div in all_divs if 'review' in ' '.join(div.get('class', [])).lower()]
                    
                    if not review_elements:
                        st.warning(f"‚ö†Ô∏è Aucune structure d'avis d√©tect√©e page {page}")
                        break
                    
                    page_reviews = 0
                    for review_element in review_elements:
                        review_data = self.extract_single_review(review_element)
                        if review_data and review_data.get('content'):
                            reviews.append(review_data)
                            page_reviews += 1
                    
                    st.success(f"‚úÖ Page {page}: {page_reviews} avis extraits")
                    
                    if page_reviews == 0:
                        st.warning("‚ö†Ô∏è Aucun avis avec contenu trouv√©, arr√™t de l'extraction")
                        break
                    
                    # V√©rifier s'il y a une page suivante
                    next_disabled = soup.select_one('li.a-disabled.a-last')
                    if next_disabled:
                        st.info("üìÑ Derni√®re page atteinte")
                        break
                        
                except Exception as e:
                    st.error(f"‚ùå Erreur page {page}: {str(e)}")
                    continue
            
            return {
                'product_info': product_info,
                'reviews': reviews,
                'asin': asin,
                'clean_url': clean_url
            }
            
        except Exception as e:
            st.error(f"‚ùå Erreur g√©n√©rale pour {original_url}: {str(e)}")
            return None

def process_batch_urls(urls, max_pages_per_product=3, progress_placeholder=None):
    """Traite une liste d'URLs en batch"""
    extractor = AmazonReviewsExtractor()
    analyzer = SentimentAnalyzer()
    results = []
    
    total_urls = len(urls)
    
    for i, url in enumerate(urls):
        if progress_placeholder:
            progress_placeholder.progress((i + 1) / total_urls)
            
        st.subheader(f"üîÑ URL {i+1}/{total_urls}")
        st.write(f"**URL originale:** {url}")
        
        # Extraction des donn√©es pour ce produit
        product_data = extractor.extract_reviews_for_product(url.strip(), max_pages_per_product)
        
        if product_data and product_data['reviews']:
            product_info = product_data['product_info'] or {}
            reviews = product_data['reviews']
            
            # Calcul des statistiques
            total_reviews = product_info.get('total_reviews', len(reviews))
            avg_rating = product_info.get('avg_rating')
            if not avg_rating and reviews:
                # Calcul de la moyenne sur les avis extraits
                ratings = [r['rating'] for r in reviews if r['rating'] is not None]
                avg_rating = sum(ratings) / len(ratings) if ratings else None
            
            # Cr√©ation d'une ligne par avis
            for review in reviews:
                if review.get('content'):  # Seulement les avis avec commentaire
                    sentiment = analyzer.analyze_sentiment(review['content'])
                    
                    results.append({
                        'url': url.strip(),
                        'nombre_avis': total_reviews,
                        'nombre_commentaires_client': len(reviews),
                        'moyenne_avis': round(avg_rating, 1) if avg_rating else None,
                        'avis_notation': review.get('rating'),
                        'commentaire_associe': review['content'],
                        'sentiment': sentiment
                    })
            
            st.success(f"‚úÖ **R√©sultat:** {len(reviews)} avis extraits avec succ√®s!")
        else:
            st.error(f"‚ùå **√âchec:** Aucun avis extrait pour cette URL")
            # Ajouter une ligne vide pour cette URL
            results.append({
                'url': url.strip(),
                'nombre_avis': 0,
                'nombre_commentaires_client': 0,
                'moyenne_avis': None,
                'avis_notation': None,
                'commentaire_associe': "Aucun avis extrait",
                'sentiment': "N/A"
            })
        
        st.markdown("---")
    
    return results

def main():
    st.title("üìù Extracteur d'avis Amazon - Version Am√©lior√©e")
    st.markdown("---")
    
    # Avertissement l√©gal
    with st.expander("‚ö†Ô∏è Avertissement important", expanded=True):
        st.warning("""
        **Utilisation responsable uniquement:**
        - Cet outil est destin√© √† un usage √©ducatif et de recherche
        - Respectez les conditions d'utilisation d'Amazon
        - Le traitement peut prendre beaucoup de temps (pauses anti-d√©tection)
        - Amazon peut bloquer les requ√™tes automatis√©es
        - Testez d'abord avec une seule URL
        """)
    
    # Mode de test pour debug
    st.sidebar.header("üõ†Ô∏è Mode Debug")
    debug_mode = st.sidebar.checkbox("Activer le mode debug (plus de logs)")
    
    # Choix du mode
    mode = st.radio(
        "üéØ Mode d'extraction:",
        ["URL unique (recommand√© pour test)", "Traitement en batch"],
        horizontal=True
    )
    
    if mode == "URL unique (recommand√© pour test)":
        st.subheader("üß™ Test avec une URL unique")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            product_url = st.text_input(
                "üîó URL du produit Amazon:",
                placeholder="https://www.amazon.fr/dp/XXXXXXXXXX",
                help="Testez d'abord avec une URL pour v√©rifier que l'extraction fonctionne"
            )
        
        with col2:
            max_pages = st.number_input(
                "üìÑ Nombre de pages max:",
                min_value=1,
                max_value=5,
                value=2,
                help="Commencez avec 1-2 pages pour tester"
            )
        
        if st.button("üöÄ Tester l'extraction", type="primary"):
            if product_url:
                with st.container():
                    st.info("üîÑ **D√©but de l'extraction de test...**")
                    
                    urls = [product_url]
                    progress_bar = st.progress(0)
                    results = process_batch_urls(urls, max_pages, progress_bar)
                    
                    if results and any(r['commentaire_associe'] != "Aucun avis extrait" for r in results):
                        df = pd.DataFrame(results)
                        
                        st.success(f"üéâ **Test r√©ussi!** {len([r for r in results if r['commentaire_associe'] != 'Aucun avis extrait'])} avis extraits!")
                        
                        # Aper√ßu des r√©sultats
                        st.subheader("üìä Aper√ßu des r√©sultats")
                        st.dataframe(df.head(3), use_container_width=True)
                        
                        # T√©l√©chargement CSV
                        csv = df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            label="üíæ T√©l√©charger CSV de test",
                            data=csv,
                            file_name=f"test_avis_amazon_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                    else:
                        st.error("‚ùå **Test √©chou√©** - Aucun avis n'a pu √™tre extrait")
                        st.info("""
                        **Causes possibles:**
                        - L'URL n'est pas valide ou ne contient pas d'avis
                        - Amazon bloque les requ√™tes automatis√©es
                        - La structure HTML a chang√©
                        - Le produit n'a pas d'avis clients
                        
                        **Solutions:**
                        - V√©rifiez que l'URL fonctionne dans votre navigateur
                        - Essayez avec une autre URL de produit
                        - Attendez quelques minutes et r√©essayez
                        """)
            else:
                st.error("‚ùå Veuillez saisir une URL")
    
    else:
        # Mode batch
        st.subheader("üìã Traitement en batch")
        st.warning("‚ö†Ô∏è **Recommandation:** Testez d'abord avec le mode 'URL unique' avant d'utiliser le batch")
        
        # Options de saisie
        input_method = st.radio(
            "Mode de saisie des URLs:",
            ["Saisie manuelle", "Upload fichier texte"],
            horizontal=True
        )
        
        urls = []
        
        if input_method == "Saisie manuelle":
            urls_text = st.text_area(
                "üîó URLs des produits Amazon (une par ligne):",
                placeholder="https://www.amazon.fr/dp/XXXXXXXXXX\nhttps://www.amazon.fr/dp/YYYYYYYYYY\n...",
                height=150
            )
            if urls_text:
                urls = [url.strip() for url in urls_text.split('\n') if url.strip()]
        
        else:
            uploaded_file = st.file_uploader(
                "üìÅ Fichier texte avec les URLs (une par ligne)",
                type=['txt']
            )
            if uploaded_file:
                content = uploaded_file.read().decode('utf-8')
                urls = [url.strip() for url in content.split('\n') if url.strip()]
        
        if urls:
            st.info(f"üìä {len(urls)} URLs d√©tect√©es")
            
            # Options pour le batch
            col1, col2 = st.columns(2)
            with col1:
                max_pages_batch = st.number_input(
                    "üìÑ Pages max par produit:",
                    min_value=1,
                    max_value=3,
                    value=1,
                    help="RECOMMAND√â: 1 page pour √©viter les blocages"
                )
            
            with col2:
                sample_urls = st.number_input(
                    "üéØ Limiter √† N URLs (0 = toutes):",
                    min_value=0,
                    max_value=len(urls),
                    value=min(5, len(urls)),
                    help="RECOMMAND√â: Commencez avec 5 URLs max"
                )
            
            # Aper√ßu des URLs
            with st.expander("üëÄ Aper√ßu des URLs √† traiter"):
                display_urls = urls[:sample_urls] if sample_urls > 0 else urls
                for i, url in enumerate(display_urls, 1):
                    st.write(f"{i}. {url}")
                if sample_urls > 0 and sample_urls < len(urls):
                    st.write(f"... et {len(urls) - sample_urls} autres URLs")
            
            # Estimation du temps
            urls_to_process = sample_urls if sample_urls > 0 else len(urls)
            estimated_time = urls_to_process * max_pages_batch * 15  # ~15 sec par page avec pauses
            st.warning(f"‚è±Ô∏è **Temps estim√©:** ~{estimated_time//60} minutes {estimated_time%60} secondes")
            st.info("üí° Le processus peut √™tre interrompu √† tout moment avec Ctrl+C")
            
            if st.button("üöÄ Lancer l'extraction en batch", type="primary"):
                if sample_urls > 0:
                    urls = urls[:sample_urls]
                
                st.warning("üïê **Extraction en cours...** Cela peut prendre plusieurs minutes")
                
                progress_bar = st.progress(0)
                results = process_batch_urls(urls, max_pages_batch, progress_bar)
                
                if results:
                    df = pd.DataFrame(results)
                    
                    # Filtrer les r√©sultats r√©ussis
                    successful_results = df[df['commentaire_associe'] != 'Aucun avis extrait']
                    
                    # Statistiques globales
                    st.subheader("üìä R√©sultats du traitement batch")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("URLs trait√©es", len(df['url'].unique()))
                    with col2:
                        st.metric("URLs r√©ussies", len(successful_results['url'].unique()))
                    with col3:
                        st.metric("Total avis", len(successful_results))
                    with col4:
                        if len(successful_results) > 0:
                            avg_rating = successful_results['avis_notation'].mean()
                            st.metric("Note moyenne", f"{avg_rating:.1f}/5" if pd.notna(avg_rating) else "N/A")
                        else:
                            st.metric("Note moyenne", "N/A")
                    
                    # R√©partition des sentiments
                    if len(successful_results) > 0:
                        st.subheader("üìà R√©partition des sentiments")
                        sentiment_counts = successful_results['sentiment'].value_counts()
                        st.bar_chart(sentiment_counts)
                    
                    # Affichage des donn√©es
                    st.subheader("üìã Donn√©es extraites")
                    st.dataframe(df, use_container_width=True)
                    
                    # T√©l√©chargement
                    csv = df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="üíæ T√©l√©charger CSV complet",
                        data=csv,
                        file_name=f"avis_amazon_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                    
                    if len(successful_results) > 0:
                        st.success(f"‚úÖ **Traitement termin√©!** {len(successful_results)} avis extraits sur {len(urls)} URLs")
                    else:
                        st.error("‚ùå **Aucun avis extrait** - Toutes les URLs ont √©chou√©")
                else:
                    st.error("‚ùå Aucun r√©sultat obtenu")

    # Instructions
    with st.expander("üìñ Format de sortie CSV"):
        st.markdown("""
        **Colonnes du fichier CSV export√©:**
        - `url`: URL du produit Amazon
        - `nombre_avis`: Nombre total d'avis pour ce produit
        - `nombre_commentaires_client`: Nombre de commentaires extraits
        - `moyenne_avis`: Note moyenne du produit (sur 5)
        - `avis_notation`: Note de cet avis sp√©cifique (1-5 √©toiles)
        - `commentaire_associe`: Texte du commentaire client
        - `sentiment`: Analyse de sentiment (Positif/N√©gatif/Neutre)
        
        **Note:** Il y a une ligne par avis/commentaire extrait.
        """)
    
    with st.expander("üîß Installation et utilisation"):
        st.markdown("""
        **Installation:**
        ```bash
        pip install -r requirements.txt
        python -m textblob.download_corpora
        streamlit run amazon_reviews_extractor.py
        ```
        
        **Conseils d'utilisation:**
        - Commencez toujours par tester une URL unique
        - Utilisez des pauses entre les extractions batch
        - Limitez le nombre de pages pour √©viter les blocages
        - Si vous √™tes bloqu√©, attendez quelques heures avant de r√©essayer
        """)

if __name__ == "__main__":
    main()
